{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GonxKillua0/Capstone-project/blob/main/StockPrice_Prediction_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual/Team\n",
        "##### AYUSH PATHAK\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focuses on analyzing and predicting stock price movements using various machine learning (ML) techniques, including both regression and classification models. The dataset comprises monthly stock prices (Open, High, Low, Close) from 2005 to 2020. The goal is to predict future stock values and price movement directions (up/down) using historical data patterns, derived metrics, and advanced ML methods.\n",
        "\n",
        "Data Preprocessing and Feature Engineering\n",
        "The dataset was initially cleaned and sorted based on the Date column. To make it analysis-ready, several new features were engineered:\n",
        "\n",
        "- Price_Change (Close - Open)\n",
        "\n",
        "- Daily_Range (High - Low)\n",
        "\n",
        "Pct_Change ((Close - Open) / Open) * 100\n",
        "These features help capture volatility, trend direction, and overall movement within each period. For classification purposes, a new binary column Price_Up was created to indicate whether the stock closed higher than it opened.\n",
        "\n",
        "Exploratory Data Analysis & Visualization\n",
        "Fifteen visualizations were developed to understand the relationships between variables. A line plot of stock closing prices revealed major volatility spikes during financial crises (2008) and the COVID crash (2020). Correlation heatmaps and pairplots showed strong linear relationships between Open, High, Low, and Close. Histograms, scatterplots, and boxplots provided insight into feature distributions and market dynamics. Notably, features like Pct_Change and Price_Change were found to be strong indicators of upward or downward movement.\n",
        "\n",
        "Model Implementation\n",
        "Two types of tasks were pursued:\n",
        "\n",
        "Regression to predict future closing prices.\n",
        "\n",
        "Classification to determine whether the stock price would increase on a given day.\n",
        "\n",
        "The models used included:\n",
        "\n",
        "1. Linear Regression\n",
        "\n",
        "2. Logistic Regression\n",
        "\n",
        "3. K-Nearest Neighbors (KNN)\n",
        "\n",
        "4. XGBoost Regressor\n",
        "\n",
        "5. XGBoost Classifier\n",
        "\n",
        "To address class imbalance in the classification task, SMOTE (Synthetic Minority Oversampling Technique) was applied.\n",
        "\n",
        "Cross-Validation & Hyperparameter Tuning\n",
        "Each model was tuned using cross-validation and GridSearchCV to find the best parameters. For instance, KNN was optimized based on the n_neighbors parameter, while XGBoost’s depth, learning rate, and number of estimators were fine-tuned to improve accuracy.\n",
        "\n",
        "Model Evaluation and Results\n",
        "The evaluation used appropriate metrics:\n",
        "\n",
        "Regression: R² and RMSE\n",
        "\n",
        "Classification: Accuracy, ROC-AUC, Confusion Matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/GonxKillua0/Capstone-project/tree/main"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build a stock price ML model"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import plotly.express as px\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from scipy import stats\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from imblearn.over_sampling import SMOTE\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Copy of data_YesBank_StockPrices.csv')"
      ],
      "metadata": {
        "id": "qPo7HawlqZQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset has no null values ."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Count - Shows that there are 185 data points for each variable.\n",
        "\n",
        "2) Mean (Average)\n",
        "Open: ₹105.54\n",
        "\n",
        "High: ₹116.10\n",
        "\n",
        "Low: ₹94.95\n",
        "\n",
        "Close: ₹105.20\n",
        "These values suggest the average price movement of the stock during the observed period.\n",
        "\n",
        "3)  Standard Deviation (std)\n",
        "Open: ₹98.88\n",
        "\n",
        "High: ₹106.33\n",
        "\n",
        "Low: ₹91.22\n",
        "\n",
        "Close: ₹98.58\n",
        "The high standard deviation values indicate significant volatility or fluctuations in the stock prices over time.\n",
        "\n",
        "4) Minimum (min)\n",
        "Open: ₹10.00\n",
        "\n",
        "High: ₹11.24\n",
        "\n",
        "Low: ₹5.55\n",
        "\n",
        "Close: ₹9.98\n",
        "These represent the lowest recorded prices during the time period. The Low price of ₹5.55 might reflect a significant dip or a cheaper stock in the dataset.\n",
        "\n",
        "5) 25th Percentile (Q1)\n",
        "Open: ₹33.80\n",
        "\n",
        "High: ₹36.14\n",
        "\n",
        "Low: ₹28.51\n",
        "\n",
        "Close: ₹33.45\n",
        "25% of the data falls below these values — often considered the lower quartile.\n",
        "\n",
        "6) Median (50%)\n",
        "Open: ₹62.98\n",
        "\n",
        "High: ₹72.55\n",
        "\n",
        "Low: ₹58.00\n",
        "\n",
        "Close: ₹62.54\n",
        "The median provides a good indication of the central tendency, less affected by extreme outliers than the mean.\n",
        "\n",
        "7) 75th Percentile (Q3)\n",
        "Open: ₹153.00\n",
        "\n",
        "High: ₹169.19\n",
        "\n",
        "Low: ₹138.35\n",
        "\n",
        "Close: ₹153.30\n",
        "75% of the data falls below these values. Indicates upper quartile, helping to understand the distribution spread.\n",
        "\n",
        "8)  Maximum (max)\n",
        "Open: ₹369.95\n",
        "\n",
        "High: ₹404.00\n",
        "\n",
        "Low: ₹345.50\n",
        "\n",
        "Close: ₹367.90\n",
        "These are the highest values recorded for each price category — showcasing possible peaks during the observation period.\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "\n",
        "# Clean and prepare dataset\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%b-%y')\n",
        "df = df.sort_values('Date')\n",
        "df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature engineering\n",
        "df['Prev_Close'] = df['Close'].shift(1)\n",
        "df['Price_Change'] = df['Close'] - df['Open']\n",
        "df['Price_Change_%'] = ((df['Close'] - df['Open']) / df['Open']) * 100\n",
        "df['Volatility'] = df['High'] - df['Low']\n",
        "df['Target'] = (df['Close'].shift(-1) > df['Close']).astype(int)\n",
        "df['Price_Up'] = np.where(df['Close'] > df['Open'],1,0)"
      ],
      "metadata": {
        "id": "hx2Ua2nwdZdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values (first and last due to shift operations)\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "vMtOTtn7da44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Converts Date to datetime\n",
        "\n",
        "2) Sorts by time\n",
        "\n",
        "3) Creates new features like volatility, price change, and target labels\n",
        "\n",
        "4) Removes missing values from shifting operations"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Insight: Shows long-term stock trends and volatility spikes (e.g., 2008, 2020)\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.lineplot(x='Date', y='Close', data=df)\n",
        "plt.title(\"Stock Closing Price Over Time\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Insight: Right-skewed distribution of prices—indicates growth with pullbacks\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(df['Close'], kde=True, bins=30)\n",
        "plt.title(\"Distribution of Closing Prices\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Insight: Close and Open have near-linear relationship; volatility visible in outliers\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x='Open', y='Close', data=df)\n",
        "plt.title(\"Open vs Close\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Insight: Greater Volatility generally relate to higher % price movements\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x='Volatility', y='Price_Change_%', data=df)\n",
        "plt.title(\" Volatilityvs % price_Change_%\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Insight: Spotting high volatility periods and large market movements\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.lineplot(x='Date', y='Price_Change_%', data=df)\n",
        "plt.title(\"% Change Over Time\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "# Insight: % change on up days is more positive; outliers show market spikes\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(x='Price_Up', y='Price_Change_%', data=df)\n",
        "plt.title(\"% Change by Price Movement Direction\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Insight: Clear difference in distribution of price change between classes\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.violinplot(x='Price_Up', y='Price_Change', data=df)\n",
        "plt.title(\"Price Change Distribution by Class\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "# Insight: Distribution of % change varies distinctly between up and down days\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(data=df, x='Price_Change_%', hue='Price_Up', kde=True, bins=40)\n",
        "plt.title(\"% Change Distribution by Class\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Insight: Mean closing price is higher on days where price increased\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x='Price_Up', y='Close', data=df)\n",
        "plt.title(\"Average Close Price by Movement Class\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "\n",
        "# Insight: High and Low are tightly coupled; linear but volatile pattern\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x='High', y='Low', data=df)\n",
        "plt.title(\"High vs Low Prices\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# Insight: Reveals price outliers and skewed distributions\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(y='Close', data=df)\n",
        "plt.title(\"Boxplot of Closing Prices\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "\n",
        "# Insight: Slightly larger ranges during upward movement days\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(x='Price_Up', y='Volatility', data=df)\n",
        "plt.title(\"Daily Range by Price Movement Direction\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Insight: Correlation matrix shows multicollinearity among prices and useful derived features\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(df[['Open', 'High', 'Low', 'Close', 'Price_Change', 'Volatility', 'Price_Change_%']].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "# Insight: Strong correlations between Open, High, Low, Close; variability in engineered features\n",
        "sns.pairplot(df[['Open', 'High', 'Low', 'Close', 'Price_Change', 'Volatility', 'Price_Change_%']])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average daily price change (Close - Open) is not equal to zero."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# H0: The average price change is 0\n",
        "# H1: The average price change is not 0\n",
        "print(\"\\n--- Hypothesis Testing: Price Change ---\")\n",
        "t_stat, p_value = stats.ttest_1samp(df['Price_Change'], 0)\n",
        "print(\"T-Statistic:\", t_stat)\n",
        "print(\"P-Value:\", p_value)\n",
        "if p_value < 0.05:\n",
        "    print(\"Result: Reject Null Hypothesis (significant change)\")\n",
        "else:\n",
        "    print(\"Result: Fail to Reject Null Hypothesis (no significant change)\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "T-statistical test is done to obtain p-value"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the t-statistical test (specifically, a one-sample t-test) to test a hypothesis about the mean of the Price_Change variabl"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average volatility (High - Low) was significantly higher during the 2008 financial crisis (2008–2009) compared to the years before it (2005–2007)."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# H0: The average volatility is not significantly higher during the crisis\n",
        "# H1: The average volatility is significantly higher\n",
        "volatility_before = df[(df['Date'] >= '2005-01-01') & (df['Date'] <= '2007-12-31')]['Volatility']\n",
        "volatility_crash = df[(df['Date'] >= '2008-01-01') & (df['Date'] <= '2009-12-31')]['Volatility']\n",
        "t_stat2, p_value2 = stats.ttest_ind(volatility_before, volatility_crash, equal_var=False)\n",
        "print(\"\\n--- Hypothesis Testing 2: Volatility During Crisis ---\")\n",
        "print(\"T-Statistic:\", t_stat2)\n",
        "print(\"P-Value:\", p_value2)\n",
        "if p_value2 < 0.05:\n",
        "    print(\"Result: Reject Null Hypothesis (volatility changed significantly during 2008-09)\")\n",
        "else:\n",
        "    print(\"Result: Fail to Reject Null Hypothesis (no significant change in volatility)\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "T-statistical test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The probability of stock price increasing the next month (Target = 1) is significantly different from random chance (50%)."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "#H0: Proportion of target = 1 is 0.5 (random)\n",
        "# H1: Proportion is not 0.5\n",
        "successes = df['Target'].sum()\n",
        "nobs = df['Target'].count()\n",
        "z_stat, p_value3 = proportions_ztest(successes, nobs, 0.5)\n",
        "print(\"\\n--- Hypothesis Testing 3: Target Movement Proportion ---\")\n",
        "print(\"Z-Statistic:\", z_stat)\n",
        "print(\"P-Value:\", p_value3)\n",
        "if p_value3 < 0.05:\n",
        "    print(\"Result: Reject Null Hypothesis (movement is not random)\")\n",
        "else:\n",
        "    print(\"Result: Fail to Reject Null Hypothesis (could be random movement)\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Z-test is used"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Check for missing values\n",
        "missing_report = df.isnull().sum()\n",
        "print(\"\\n--- Missing Values Report ---\")\n",
        "print(missing_report[missing_report > 0])\n",
        "\n",
        "# Fill missing numeric values with forward fill followed by backfill (as fallback)\n",
        "df.fillna(method='ffill', inplace=True)\n",
        "df.fillna(method='bfill', inplace=True)\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "def remove_outliers(df, columns):\n",
        "    for col in columns:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower = Q1 - 1.5 * IQR\n",
        "        upper = Q3 + 1.5 * IQR\n",
        "        df = df[(df[col] >= lower) & (df[col] <= upper)]\n",
        "    return df\n",
        "\n",
        "numeric_cols = ['Open', 'High', 'Low', 'Close']\n",
        "df = remove_outliers(df, numeric_cols)"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "df['Month'] = df['Date'].dt.month"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "def preprocess_text(text):\n",
        "  lowercased = expanded.lower()\n",
        "  return lowercased"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "def remove_punctuation(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub('', text)"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "def remove_whitespace(text):\n",
        "    return ' '.join(text.split())"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "def  normalize_text(text):\n",
        "  stemmer = PorterStemmer()\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  words = word_tokenize(text)\n",
        "  stemmed_words = [stemmer.stem(word) for word in words]\n",
        "  lemmatized_words = [lemmatizer.lemmatize(word) for word in words]"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "def pos_tagging(text):\n",
        "    words = word_tokenize(text)\n",
        "    return nltk.pos_tag(words)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "def vectorize_text(text):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    return vectorizer.fit_transform(text)"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "df['Prev_Close'] = df['Close'].shift(1)\n",
        "df['Price_Change'] = df['Close'] - df['Open']\n",
        "df['Price_Change_%'] = ((df['Close'] - df['Open']) / df['Open']) * 100\n",
        "df['Volatility'] = df['High'] - df['Low']\n",
        "df['Target'] = (df['Close'].shift(-1) > df['Close']).astype(int)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Selection\n",
        "features = ['Open', 'High', 'Low', 'Price_Change', 'Volatility', 'Price_Change_%']\n",
        "X = df[features]\n",
        "y_reg = df['Close']  # Regression target\n",
        "y_clf = df['Price_Up']  # Classification target"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X, y_clf, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "uTjmbPpY2s0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_r = scaler.fit_transform(X_train_r)\n",
        "X_test_r = scaler.transform(X_test_r)\n",
        "X_train_c = scaler.fit_transform(X_train_c)\n",
        "X_test_c = scaler.transform(X_test_c)\n",
        "\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X, y_clf, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train size 80%\n",
        "\n",
        "test size 20%"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = []\n",
        "eval_scores = []"
      ],
      "metadata": {
        "id": "sIwOwCf6yEQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# KNN with GridSearchCV\n",
        "knn_params = {'n_neighbors': list(range(3, 15))}\n",
        "gs_knn = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5)\n",
        "gs_knn.fit(X_train_c, y_train_c)\n",
        "print(\"\\nBest KNN Params:\", gs_knn.best_params_)\n",
        "y_pred_knn = gs_knn.predict(X_test_c)\n",
        "knn_acc = accuracy_score(y_test_c, y_pred_knn)\n",
        "print(\"KNN Accuracy:\", knn_acc)\n",
        "print(confusion_matrix(y_test_c, y_pred_knn))\n",
        "model_names.append(\"KNN\")\n",
        "eval_scores.append([\"Accuracy\", knn_acc])\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "log_model = LogisticRegression()\n",
        "log_cv = cross_val_score(log_model, X_train_c, y_train_c, cv=5, scoring='accuracy')\n",
        "log_model.fit(X_train_c, y_train_c)\n",
        "y_pred_log = log_model.predict(X_test_c)\n",
        "log_acc = accuracy_score(y_test_c, y_pred_log)\n",
        "log_auc = roc_auc_score(y_test_c, log_model.predict_proba(X_test_c)[:,1])\n",
        "print(\"\\nLogistic Regression Accuracy:\", log_acc)\n",
        "print(\"Logistic Regression CV Accuracy:\", log_cv)\n",
        "print(classification_report(y_test_c, y_pred_log))\n",
        "model_names.append(\"Logistic Regression\")\n",
        "eval_scores.append([\"Accuracy\", log_acc])\n",
        "eval_scores.append([\"ROC-AUC\", log_auc])\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "# ROC-AUC for Logistic Regression\n",
        "log_probs = log_model.predict_proba(X_test_c)[:, 1]\n",
        "fpr, tpr, thresholds = roc_curve(y_test_c, log_probs)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(fpr, tpr, label=\"Logistic ROC Curve\")\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - Logistic Regression\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2]\n",
        "}\n",
        "gs_xgb = GridSearchCV(xgb_clf, xgb_params, cv=5)\n",
        "gs_xgb.fit(X_train_c, y_train_c)\n",
        "print(\"\\nBest XGBoost Classifier Params:\", gs_xgb.best_params_)\n",
        "y_pred_xgb_clf = gs_xgb.predict(X_test_c)\n",
        "xgb_acc = accuracy_score(y_test_c, y_pred_xgb_clf)\n",
        "print(\"XGBoost Classifier Accuracy:\", xgb_acc)\n",
        "model_names.append(\"XGBoost Classifier\")\n",
        "eval_scores.append([\"Accuracy\", xgb_acc])\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Feature importance from XGBoost\n",
        "plt.figure(figsize=(8,6))\n",
        "importances = gs_xgb.best_estimator_.feature_importances_\n",
        "sns.barplot(x=importances, y=features)\n",
        "plt.title(\"Feature Importance - XGBoost Classifier\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "xgb_reg = XGBRegressor()\n",
        "xgb_cv = cross_val_score(xgb_reg, X_train_r, y_train_r, cv=5, scoring='r2')\n",
        "xgb_reg.fit(X_train_r, y_train_r)\n",
        "y_pred_xgb_reg = xgb_reg.predict(X_test_r)\n",
        "xgb_r2 = r2_score(y_test_r, y_pred_xgb_reg)\n",
        "xgb_rmse = np.sqrt(mean_squared_error(y_test_r, y_pred_xgb_reg))\n",
        "print(\"\\nXGBoost Regressor R^2:\", xgb_r2)\n",
        "print(\"XGBoost Regressor RMSE:\", xgb_rmse)\n",
        "print(\"XGBoost Regressor CV R^2 Scores:\", xgb_cv)\n",
        "model_names.append(\"XGBoost Regressor\")\n",
        "eval_scores.append([\"R^2\", xgb_r2])\n",
        "eval_scores.append([\"RMSE\", xgb_rmse])\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost Regressor is the optimal choice for continuous stock price prediction because of its strong performance on all key evaluation metrics, cross-validation stability, and ability to model non-linear relationships in stock market data."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the evaluation metrics:\n",
        "\n",
        "1. XGBoost Regressor is selected as the final model for predicting stock prices.\n",
        "\n",
        "2. It consistently achieved the highest R² scores, lowest RMSE, and demonstrated strong performance in cross-validation.\n",
        "\n",
        "3. Its ability to model non-linear relationships and perform well on large datasets makes it more robust compared to Linear Regression.\n",
        "\n",
        "\n",
        "This project demonstrates that machine learning can be a powerful tool for financial analysis when combined with domain-specific features and robust evaluation methods. XGBoost stood out as the most reliable model due to its accuracy, flexibility, and ability to handle non-linear relationships. Future enhancements could include incorporating technical indicators (e.g., moving averages, RSI), sentiment analysis, and real-time data to improve predictive power.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}